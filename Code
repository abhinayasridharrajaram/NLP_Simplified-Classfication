import numpy as np
import pandas as pd
pd.options.display.float_format = '{:.2f}'.format
import os
print(os.getcwd())
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import plotly.express as px
import plotly.offline as ply
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly.tools import make_subplots
import warnings
import sys
import dtale
import autoviz
import pandas_profiling
!{sys.executable} -m pip install dtale
warnings.filterwarnings('ignore')
import sys
!{sys.executable} -m pip install pandas-profiling
import os
import sys
!{sys.executable} -m pip install pyppeteer
import sys
!{sys.executable} -m pip install spacy
import sys
!{sys.executable} -m pip install nltk

import seaborn as sns
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import notebook as pdf
import pyppeteer
import sys
!{sys.executable} -m pip install dython

import sys
!{sys.executable} -m pip install tabulate

import sys
!{sys.executable} -m pip install phonenumbers
from tabulate import tabulate
import sys
!{sys.executable} -m pip install calplot

import sys
!{sys.executable} -m pip install weasyprint


import sys
!{sys.executable} -m pip install pytextrank

import sys
!{sys.executable} -m pip install spacy

import sys
!{sys.executable} -m spacy download en_core_web_sm
import nltk
from nltk.util import ngrams
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from nltk.classify import MaxentClassifier
from nltk.stem.snowball import SnowballStemmer
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.feature_extraction.text import TfidfTransformer





# Count of target variable

f = open('NP_CJ_train_.csv', 'rU')
# Initialize the lists for X and Y
data = pd.read_csv('NP_CJ_train_.csv')
  
#print(data)

  
plt.figure(figsize=(8,5))
sns.countplot(x ='CJ_Mission?',data= data, palette='rainbow')

plt.show()

# Word Count

data['word_count'] = data['Mission Statement'].apply(lambda x: len(str(x).split()))
print(' Avg words used in  Mission Statement column of Criminal Justice non-profits is ',data[data['CJ_Mission?']=='Yes']['word_count'].mean()) #Disaster tweets
print('Avg words used in  Mission Statement column of NON- Criminal Justice non-profits is',data[data['CJ_Mission?']=='No']['word_count'].mean())
# PLOTTING WORD-COUNT
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))
train_words=data[data['CJ_Mission?']== 'Yes']['word_count']
ax1.hist(train_words,color='red')
ax1.set_title('Criminal Justice')
train_words=data[data['CJ_Mission?']== 'No']['word_count']
ax2.hist(train_words,color='green')
ax2.set_title('Non-Criminal-Justice')
fig.suptitle('Words per Mission Statement')
plt.show()

# Define a standard snowball stemmer
STEM = SnowballStemmer('english')
# Make a list of stopwords, including the stemmed versions
# These are words that have no impact on the classification, and
# can even occasionally mess up the classifier.

STOP = ['in',
    'non profit',
    'from',
    'the',
    'and',
    'their', 
    'after',
    'for',
    'in',
    'that',
    'our',
    'we',
    'that',
    'mission',
    'our',
    'in ',
    'of',
    'to',
    'a', 'people', 'IN', 'social', 'community' 'BY', 'OF', 'IN',
    'and',
    'to',
    'the',
    'of ',
    'AND',
    'the',
    'THE',
    'and',
    'in ',
    'as',
    'is',
    'by',
    'of',
    'to',
    'a', 'CIVIL', 'organization'
]


STOP += [STEM.stem(i) for i in STOP]
print(STOP)
STOP = list(set(STOP))
print(STOP)


def tokenize(desc):
    """
    Takes description text, strips out unwanted words and text,
    and prepares it for the trainer.
    """
    # first lower case and strip leading/trailing whitespace
    desc = desc.lower().strip()
    # kill the 'do-'s and any stray punctuation
    desc = desc.replace('do-', '').replace('.', '').replace(',', '')
    # make a list of words by splitting on whitespace
    words = desc.split(' ')
    # Make sure each "word" is a real string / account for odd whitespace
    words = [STEM.stem(i) for i in words if i]
    
    words = [i for i in words if i not in STOP]
    # let's see if adding bigrams improves the accuracy
    bigrams = ngrams(words, 2)
    bigrams = ["%s|%s" % (i[0], i[1]) for i in bigrams]
    # bigrams = [i for i in bigrams if STEMMED_BIGRAMS.get(i)]
    # set up a dict
    output = dict([(i, True) for i in words + bigrams])
    # The NLTK trainer expects data in a certain format
    return output
    

# open our sample file and use the CSV module to parse it
f = open('NP_CJ_train_.csv', 'rU')
data = list(csv.DictReader(f))

# Make an empty list for our processed data
qualities  = []
# Loop through all the lines in the CSV
for i in data:
    desc = i.get('Mission Statement')
    classify = i.get('CJ_Mission?')
    feats = tokenize(desc)
    qualities.append((feats, classify))
    

f.close()


# Train our classifiers. Let's start with Linear SVC
# Make a data prep pipeline
pipeline = Pipeline([
    ('tfidf', TfidfTransformer()),
    ('linearsvc', LinearSVC()),
])
# make the classifier
linear_svc = SklearnClassifier(pipeline)
# Train it
linear_svc.train(qualities)

# Here's what this looks like
print (qualities[0])


maxent = MaxentClassifier.train(qualities)


test_data = list(csv.DictReader(open('NP_CJ_test_.csv', 'rU')))
d =[]
for i in test_data:
    desc = i.get('Mission Statement')
    
    classify = i.get('CJ_Mission?')
    
    tokenized = tokenize(desc)
    
    # now grab the results of our classifiers
    maxent_class = maxent.classify(tokenized)
  
    
    svc_class = linear_svc.classify(tokenized)
    # Generate result using pandas
    print('actual: %s | maxent: %s | linear_svc: %s |' % (classify, maxent_class, svc_class))
    
   
    d.append(( classify,maxent_class, svc_class))
   
    #print(d[-1])
    
 #print(d)
df2 = pd.DataFrame(d)
df2.columns = ['Correct_label', 'maxent_predict', 'svc_predict']
#print (df2)
contingency_matrix = pd.crosstab(df2['svc_predict'], df2['Correct_label'])

print(contingency_matrix)

from sklearn import metrics
target_names = ['Yes', 'No']

print("SVC", classification_report(df2.Correct_label,df2.svc_predict , target_names=target_names))
print("Maxent", classification_report(df2.Correct_label,df2.maxent_predict , target_names=target_names))


import matplotlib.pyplot as plt
import seaborn as sn

plt.clf()

ax = fig.add_subplot(111)

ax.set_aspect(1)

res = sn.heatmap(contingency_matrix.T, annot=True, fmt='.2f', cmap="YlGnBu", cbar=False)

plt.savefig("crosstab_pandas.png", bbox_inches='tight', dpi=100)
plt.title(' Confusion Matrix  for SVC')
plt.show()

